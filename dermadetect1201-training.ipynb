{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Image Classification Models\n",
    "\n",
    "This notebook trains an Image Classification model for skin conditions and rashes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Here we define S3 file paths for input and output data, the training image containing the semantic segmentaion algorithm, and instantiate a SageMaker session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import re\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "import json\n",
    "\n",
    "role = get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "training_image = sagemaker.amazon.amazon_estimator.get_image_uri(boto3.Session().region_name, 'image-classification', repo_version='latest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide inputs for training and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_manifest_filename_train = 'train.manifest' \n",
    "augmented_manifest_filename_validation = 'validation.manifest' # Replace with the filename for your validation data.\n",
    "bucket_name = \"sciencemit\" # Replace with your bucket name.\n",
    "s3_prefix = '' # Replace with the S3 prefix where your data files reside.\n",
    "s3_output_path = 's3://{}/output'.format(bucket_name) # Replace with your desired output directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The setup section concludes with a few more definitions and constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines paths for use in the training job request.\n",
    "s3_train_data_path = 's3://{}/{}'.format(bucket_name, augmented_manifest_filename_train)\n",
    "s3_validation_data_path = 's3://{}/{}'.format(bucket_name, augmented_manifest_filename_validation)\n",
    "\n",
    "print(\"Augmented manifest for training data: {}\".format(s3_train_data_path))\n",
    "print(\"Augmented manifest for validation data: {}\".format(s3_validation_data_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview Input Data\n",
    "\n",
    "Let's read the augmented manifest so we can inspect its contents to better understand the format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_manifest_s3_key = s3_train_data_path.split(bucket_name)[1][1:]\n",
    "s3_obj = s3.Object(bucket_name, augmented_manifest_s3_key)\n",
    "augmented_manifest = s3_obj.get()['Body'].read().decode('utf-8')\n",
    "augmented_manifest_lines = augmented_manifest.split('\\n')\n",
    "\n",
    "num_training_samples = len(augmented_manifest_lines) # Compute number of training samples for use in training job request.\n",
    "\n",
    "\n",
    "print('Preview of Augmented Manifest File Contents')\n",
    "print('-------------------------------------------')\n",
    "print('\\n')\n",
    "\n",
    "for i in range(2):\n",
    "    print('Line {}'.format(i+1))\n",
    "    print(augmented_manifest_lines[i])\n",
    "    print('\\n')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we know the importance of the `AttributeNames` parameter in the training job request, let's go ahead and define it so that it corresponds to what we've seen in this example augmented manifest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute_names = [\"source-ref\",\"job-science-1201\"] # Replace as appropriate for your augmented manifest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Training Job\n",
    "\n",
    "First, we'll construct the request for the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create unique job name \n",
    "job_name_prefix = 'job-science-1201'\n",
    "timestamp = time.strftime('-%Y-%m-%d-%H-%M-%S', time.gmtime())\n",
    "job_name = job_name_prefix + timestamp\n",
    "\n",
    "training_params = \\\n",
    "{\n",
    "    \"AlgorithmSpecification\": {\n",
    "        \"TrainingImage\": training_image, # NB. This is one of the named constants defined in the first cell.\n",
    "        \"TrainingInputMode\": \"Pipe\"\n",
    "    },\n",
    "    \"RoleArn\": role,\n",
    "    \"OutputDataConfig\": {\n",
    "        \"S3OutputPath\": s3_output_path\n",
    "    },\n",
    "    \"ResourceConfig\": {\n",
    "        \"InstanceCount\": 1,   \n",
    "        \"InstanceType\": \"ml.p3.2xlarge\",\n",
    "        \"VolumeSizeInGB\": 50\n",
    "    },\n",
    "    \"TrainingJobName\": job_name,\n",
    "    \"HyperParameters\": { # NB. These hyperparameters are at the user's discretion and are beyond the scope of this demo.\n",
    "        \"image_shape\": \"3,224,224\",\n",
    "        \"num_training_samples\": \"280\",\n",
    "        \"num_classes\": \"11\",\n",
    "        \"mini_batch_size\": \"32\",\n",
    "        \"epochs\": \"30\",\n",
    "        \"learning_rate\": \"0.1\",\n",
    "        \"num_layers\": \"101\"\n",
    "    },\n",
    "    \"StoppingCondition\": {\n",
    "        \"MaxRuntimeInSeconds\": 86400\n",
    "    },\n",
    "    \"InputDataConfig\": [\n",
    "        {\n",
    "            \"ChannelName\": \"train\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"AugmentedManifestFile\", # NB. Augmented Manifest\n",
    "                    \"S3Uri\": s3_train_data_path,\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\",\n",
    "                    \"AttributeNames\": attribute_names # NB. This must correspond to the JSON field names in your augmented manifest.\n",
    "                }\n",
    "            },\n",
    "            \"ContentType\": \"application/x-recordio\",\n",
    "            \"RecordWrapperType\": \"RecordIO\",\n",
    "            \"CompressionType\": \"None\"\n",
    "        },\n",
    "        {\n",
    "            \"ChannelName\": \"validation\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"AugmentedManifestFile\", # NB. Augmented Manifest\n",
    "                    \"S3Uri\": s3_validation_data_path,\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\",\n",
    "                    \"AttributeNames\": attribute_names # NB. This must correspond to the JSON field names in your augmented manifest.\n",
    "                }\n",
    "            },\n",
    "            \"ContentType\": \"application/x-recordio\",\n",
    "            \"RecordWrapperType\": \"RecordIO\",\n",
    "            \"CompressionType\": \"None\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    " \n",
    "print('Training job name: {}'.format(job_name))\n",
    "print('\\nInput Data Location: {}'.format(training_params['InputDataConfig'][0]['DataSource']['S3DataSource']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the Amazon SageMaker training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.client(service_name='sagemaker')\n",
    "client.create_training_job(**training_params)\n",
    "\n",
    "# Confirm that the training job has started\n",
    "status = client.describe_training_job(TrainingJobName=job_name)['TrainingJobStatus']\n",
    "print('Training job current status: {}'.format(status))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainingJobStatus = client.describe_training_job(TrainingJobName=job_name)['TrainingJobStatus']\n",
    "SecondaryStatus = client.describe_training_job(TrainingJobName=job_name)['SecondaryStatus']\n",
    "print(TrainingJobStatus, SecondaryStatus)\n",
    "while TrainingJobStatus !='Completed' and TrainingJobStatus!='Failed':\n",
    "    time.sleep(60)\n",
    "    TrainingJobStatus = client.describe_training_job(TrainingJobName=job_name)['TrainingJobStatus']\n",
    "    SecondaryStatus = client.describe_training_job(TrainingJobName=job_name)['SecondaryStatus']\n",
    "    print(TrainingJobStatus, SecondaryStatus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_info = client.describe_training_job(TrainingJobName=job_name)\n",
    "print(training_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference and Predictions\n",
    "\n",
    "We now deploy the Image Classification model with an endpoint and perform inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import boto3\n",
    "from time import gmtime, strftime\n",
    "import re\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "sage = boto3.Session().client(service_name='sagemaker') \n",
    "\n",
    "model_name=\"job-science-1201-model-3\"\n",
    "print(model_name)\n",
    "info = sage.describe_training_job(TrainingJobName=job_name)\n",
    "model_data = info['ModelArtifacts']['S3ModelArtifacts']\n",
    "print(model_data)\n",
    "\n",
    "hosting_image = get_image_uri(boto3.Session().region_name, 'image-classification')\n",
    "\n",
    "primary_container = {\n",
    "    'Image': hosting_image,\n",
    "    'ModelDataUrl': model_data,\n",
    "}\n",
    "\n",
    "create_model_response = sage.create_model(\n",
    "    ModelName = model_name,\n",
    "    ExecutionRoleArn = role,\n",
    "    PrimaryContainer = primary_container)\n",
    "\n",
    "print(create_model_response['ModelArn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create Endpoint**\n",
    "\n",
    "Next, I create the endpoint that serves up the image classification model, through specifying the name and configuration defined above. The end result is an endpoint that can be validated and incorporated into applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "\n",
    "timestamp = time.strftime('-%Y-%m-%d-%H-%M-%S', time.gmtime())\n",
    "endpoint_config_name = job_name_prefix + '-epc-' + timestamp\n",
    "endpoint_config_response = sage.create_endpoint_config(\n",
    "    EndpointConfigName = endpoint_config_name,\n",
    "    ProductionVariants=[{\n",
    "        'InstanceType':'ml.m4.xlarge',\n",
    "        'InitialInstanceCount':1,\n",
    "        'ModelName':model_name,\n",
    "        'VariantName':'AllTraffic'}])\n",
    "\n",
    "print('Endpoint configuration name: {}'.format(endpoint_config_name))\n",
    "print('Endpoint configuration arn:  {}'.format(endpoint_config_response['EndpointConfigArn']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import time\n",
    "\n",
    "timestamp = time.strftime('-%Y-%m-%d-%H-%M-%S', time.gmtime())\n",
    "endpoint_name = job_name_prefix + '-ep-' + timestamp\n",
    "print('Endpoint name: {}'.format(endpoint_name))\n",
    "\n",
    "endpoint_params = {\n",
    "    'EndpointName': endpoint_name,\n",
    "    'EndpointConfigName': endpoint_config_name,\n",
    "}\n",
    "endpoint_response = sage.create_endpoint(**endpoint_params)\n",
    "print('EndpointArn = {}'.format(endpoint_response['EndpointArn']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the status of the endpoint\n",
    "response = sage.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = response['EndpointStatus']\n",
    "print('EndpointStatus = {}'.format(status))\n",
    "\n",
    "\n",
    "# wait until the status has changed\n",
    "sage.get_waiter('endpoint_in_service').wait(EndpointName=endpoint_name)\n",
    "\n",
    "\n",
    "# print the status of the endpoint\n",
    "endpoint_response = sage.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = endpoint_response['EndpointStatus']\n",
    "print('Endpoint creation ended with EndpointStatus = {}'.format(status))\n",
    "\n",
    "if status != 'InService':\n",
    "    raise Exception('Endpoint creation failed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perform Inference**\n",
    "\n",
    "Now we can validate the model for use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "runtime = boto3.Session().client(service_name='runtime.sagemaker') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Download test image**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import imageio\n",
    "import numpy as np\n",
    "import imageio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as rocplt\n",
    "import scikitplot as skplt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "from collections import namedtuple\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# Load the output manifest's annotations.\n",
    "OUTPUT_MANIFEST = 's3://sciencemit/output.manifest' # Replace with the S3 URI for your output manifest.\n",
    "!aws s3 cp {OUTPUT_MANIFEST} 'output.manifest'\n",
    "\n",
    "with open('output.manifest', 'r') as f:\n",
    "    output = [json.loads(line.strip()) for line in f.readlines()]\n",
    "\n",
    "# Create data arrays.\n",
    "img_uris = [None] * len(output)\n",
    "confidences = np.zeros(len(output))\n",
    "labels = [None] * len(output)\n",
    "human = np.zeros(len(output))\n",
    "\n",
    "# Find the job name the manifest corresponds to.\n",
    "keys = list(output[0].keys())\n",
    "metakey = keys[np.where([('-metadata' in k) for k in keys])[0][0]]\n",
    "jobname = metakey[:-9]\n",
    "\n",
    "# Extract the data.\n",
    "for datum_id, datum in enumerate(output):\n",
    "    img_uris[datum_id] = datum['source-ref']\n",
    "    labels[datum_id] = datum[metakey]['class-name']\n",
    "    confidences[datum_id] = datum[metakey]['confidence']\n",
    "    human[datum_id] = int(datum[metakey]['human-annotated'] == 'yes')\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Compute the number of annotations in each class.\n",
    "n_classes = len(set(labels))\n",
    "sorted_clnames, class_sizes = zip(*Counter(labels).most_common(n_classes))\n",
    "\n",
    "# Find ids of human-annotated images.\n",
    "human_sizes = [human[labels == clname].sum() for clname in sorted_clnames]\n",
    "class_sizes = np.array(class_sizes)\n",
    "human_sizes = np.array(human_sizes)\n",
    "\n",
    "# Compute the average annotation confidence per class.\n",
    "human_confidences = np.array([confidences[np.logical_and(labels == clname, human)]\n",
    "                              for clname in sorted_clnames])\n",
    "machine_confidences = [confidences[np.logical_and(labels == clname, 1-human)]\n",
    "                       for clname in sorted_clnames]\n",
    "\n",
    "# If there is no images annotated as a specific class, set the average class confidence to 0.\n",
    "for class_id in range(n_classes):\n",
    "    if human_confidences[class_id].size == 0:\n",
    "        human_confidences[class_id] = np.array([0])\n",
    "    if machine_confidences[class_id].size == 0:\n",
    "        machine_confidences[class_id] = np.array([0])\n",
    "\n",
    "LOCAL_IMG_DIR = './dataset' # Replace with the name of a local directory to store images.\n",
    "DATASET_SIZE = len(img_uris) # Change this to a reasonable number if your dataset much larger than 10K images.\n",
    "\n",
    "subset_ids = np.random.choice(range(len(img_uris)), DATASET_SIZE, replace=False)\n",
    "img_uris = [img_uris[idx] for idx in subset_ids]\n",
    "\n",
    "labels = labels[subset_ids]\n",
    "confidences = confidences[subset_ids]\n",
    "human = human[subset_ids]\n",
    "\n",
    "img_fnames = [None] * len(output)\n",
    "for img_uri_id, img_uri in enumerate(img_uris):\n",
    "    target_fname = os.path.join(\n",
    "        LOCAL_IMG_DIR, img_uri.split('/')[-1])\n",
    "    if not os.path.isfile(target_fname):\n",
    "        !aws s3 cp {img_uri} {target_fname}\n",
    "    img_fnames[img_uri_id] = target_fname\n",
    "    \n",
    "N_SHOW = 15\n",
    "mean_prediction_classes = []\n",
    "\n",
    "plt.figure(figsize=(3 * N_SHOW, 2 + 3 * n_classes), facecolor='white', dpi=60)\n",
    "for class_name_id, class_name in enumerate(sorted_clnames):\n",
    "    \n",
    "    class_prob = 0\n",
    "    num_ids = 0\n",
    "    \n",
    "    class_ids = np.where(np.logical_and(np.array(labels) == class_name, human))[0]\n",
    "    actual_class.append(class_name)\n",
    "    try:\n",
    "        show_ids = np.random.choice(class_ids, N_SHOW, replace=False)\n",
    "    except ValueError:\n",
    "        print('Not enough human annotations to show {}'.format(class_name))\n",
    "        continue\n",
    "    for show_id_id, show_id in enumerate(show_ids):\n",
    "    \n",
    "        file_name = img_fnames[show_id]\n",
    "        \n",
    "        with open(file_name, 'rb') as f:\n",
    "            payload = f.read()\n",
    "            payload = bytearray(payload)\n",
    "        try:\n",
    "            response = runtime.invoke_endpoint(EndpointName=endpoint_name, \n",
    "                                   ContentType='application/x-image', \n",
    "                                   Body=payload)\n",
    "        except:\n",
    "            continue\n",
    "        result = response['Body'].read()\n",
    "        # result will be in json format and convert it to ndarray\n",
    "        result = json.loads(result)\n",
    "\n",
    "        # the result will output the probabilities for all classes\n",
    "        # find the class with maximum probability and print the class index\n",
    "        index = np.argmax(result)\n",
    "\n",
    "        object_categories = ['chicken pox', 'heat rash', 'impetigo', 'eczema', 'measles', 'hives', 'baby acne', 'dermatitis', 'fifth disease', 'cold sores', 'scarlet fever']\n",
    "        probability = result[index]\n",
    "            \n",
    "        class_prob = class_prob + (probability*7)\n",
    "        num_ids = num_ids + 1\n",
    "    \n",
    "        print('Ground truth: ', class_name, ' Actual probability: ',str(confidences[show_id_id]), ' Predicted class: ', object_categories[index], ' Prediction: ' + str(probability*7))\n",
    "\n",
    "    print('Class name: ', class_name, ' ', 'Mean predicted probability: ', str((class_prob/num_ids))\n",
    "    mean_prediction_classes.append(class_prob/num_ids)\n",
    "\n",
    "classes = ['chicken pox', 'heat rash', 'impetigo', 'eczema', 'measles', 'hives', 'baby acne', 'dermatitis', 'fifth disease', 'cold sores', 'scarlet fever']\n",
    "y_pos = np.arange(len(classes))\n",
    " \n",
    "plt.figure(figsize=(9, 3), facecolor='white', dpi=100)\n",
    "plt.bar(y_pos, mean_prediction_classes, align='center', alpha=0.5)\n",
    "plt.xticks(y_pos, classes, rotation=90)\n",
    "plt.ylabel('predicted confidences')\n",
    "plt.title('Mean of predicted confidences')\n",
    " \n",
    "plt.show()\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O /tmp/test.jpg https://assets.nhs.uk/prod/images/D1BMFN.2e16d0ba.fill-920x613.jpg\n",
    "file_name = '/tmp/test.jpg'\n",
    "# test image\n",
    "from IPython.display import Image\n",
    "Image(file_name)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation**\n",
    "\n",
    "Evaluate the image through the network for inteference. The network outputs class probabilities and typically, one selects the class with the maximum probability as the final class output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sage.delete_endpoint(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
